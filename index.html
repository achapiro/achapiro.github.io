
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Color scheme stolen from Sergey Karayev */
  a {
    color: #1772d0;
    text-decoration:none;
  }
  a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
  }
  body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
  }
  strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
  }
  heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
  }
  papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
  }
  name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
  }
  .one
  {
    width: 160px;
    height: 160px;
    position: relative;
  }
  .two
  {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
  }
  .fade {
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
  }
  span.highlight {
    background-color: #ffffd0;
  }
  </style>
  <link rel="icon" type="image/png" href="seal_icon.png">
  <title>Alex Chapiro</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>
<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Alex Chapiro</name>
              </p>
              <p>I am an imaging architect and senior staff research scientist at <a href="https://www.oculus.com/research/">Meta</a>, working in <a href="https://www.linkedin.com/in/ajitninan/">Ajit Ninan's</a> Imaging Experiences Architecture team in Reality Labs. I previously worked in the Core Display Incubation team at Apple, the Applied Vision Science team at Dolby Laboratories, and the Stereo and Displays group at Disney Research Zurich.
              </p>
              <p>
                I did my PhD in Markus Gross' <a href="https://cgl.ethz.ch/">Computer Graphics Laboratory</a> at <a href="http://ethz.ch/">ETH Zurich</a>. I got my master's in applied math at <a href="http://www.impa.br/opencms/pt/">IMPA</a>/<a href="http://www.visgraf.impa.br/home/">Visgraf</a>, and my bachelors in math at the <a href="http://www.ufjf.br/ufjf/">Federal University of Juiz de Fora</a>.
              </p>
              <p align=center>
                <a href="mailto:alex@chapiro.net">Email</a> &nbsp/&nbsp
                <a href="AlexChapiro-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="AlexChapiro-bio.txt">Biography</a> &nbsp/&nbsp
                <a href="AlexChapiro-thesis.pdf">Thesis</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=v56FzIAAAAAJ&h">Google Scholar</a> &nbsp/&nbsp
                <a href="http://www.linkedin.com/in/alexandre-chapiro-a8342520"> LinkedIn </a>
              </p>
            </td>
            <td width="33%">
              <img src="Alex_orion_headshot.png", width=110%>
            </td>

          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
		I am interested in perception and computer graphics, especially anything involving computational display and psychophysics. Prior work involved perceptual metrics, brightness and color, stereo 3D, and display topics like virtual and augmented reality, frame rate, high dynamic range and more.              </p>
            </td>
          </tr>
        </table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">


	  <tr>
            <td width="25%"><img src="Ash25/Ash25.png" alt="Nature Comms'25" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Ash25/Ash25.pdf"  id="MCG_journal">
                    <papertitle>Resolution Limit of the Eye: How Many Pixels Can We See?</papertitle>
                  </a>
                  <br>
                  Ashraf, <strong>Chapiro</strong>, Mantiuk
                  <br><em>Nature Communications 2025</em>
		  <br><a href="#">project page </a> /
		  <a href="https://github.com/gfxdisp/resolution_limit/">GitHub </a> /
                </p>
                <p>
                  We investigated the "retinal resolution" of the human visual system. To do this, we built a custom display rig that allowed us to change effective resolution continuously. We collected data for foveal and peripheral, achromatic and chromatic stimuli. Our results set the north star for display development, with implications for future imaging, rendering and video coding technologies.
				</p>
              </td>
            </tr>
		
	  <tr>
            <td width="25%"><img src="Kim25/Kim25.png" alt="SIGGRAPH'25" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Kim25/Kim25.pdf"  id="MCG_journal">
                    <papertitle>Supra-threshold Contrast Perception in Augmented Reality</papertitle>
                  </a>
                  <br>
                  Kim, Ashraf, <strong>Chapiro</strong>, Mantiuk
                  <br><em>SIGGRAPH Asia 2025 [conference]</em>
		  <br><a href="https://www.cl.cam.ac.uk/research/rainbow/projects/ar_contrast/">project page </a> /
		  <a href="Kim25/Kim25_supplementary.pdf">supplementary </a> /
		  <a href="https://github.com/gfxdisp/ar_contrast_perception">GitHub </a> /
		  <a href="https://www.youtube.com/watch?v=tPUkpA6VI8c">video</a>
                </p>
                <p>
                  We studied the perception of contrast in Augmented Reality. In particular, we were interested in understanding and modeling how AR image visibility in the presence of an additive background differs from traditional display. We found that existing supra-threshold contrast sensitivity models do a good job of explaining the effect, influenced by threshold elevation with added background luminance. 
		</p>
              </td>
            </tr>		

	  <tr>
            <td width="25%"><img src="Sah25/Sah25.jpg" alt="SIGGRAPH'25" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Sah25/Sah25.pdf"  id="MCG_journal">
                    <papertitle>FaceExpressions-70k: A Dataset of Perceived Expression Differences</papertitle>
                  </a>
                  <br>
                  Saha, Chen, Bazin, Hane, Katsavounidis, <strong>Chapiro</strong>, Bovik
                  <br><em>SIGGRAPH 2025 [conference]</em>
		  <br><a href="https://avinabsaha.github.io/face-expressions-70k/">project page </a> /
		  <a href="https://github.com/avinabsaha/face-expressions-70k">GitHub </a> /
		  <a href="https://www.youtube.com/watch?v=PSSDaqYTyKY">video</a>
                </p>
                <p>
                  We ran a large-scale user study to quantify the perceived differences between facial expressions. Our dataset covers a diverse range of expressions, and measures both inter and intra-expression differences on multiple actors.
		</p>
              </td>
            </tr>
				


		

	  <tr>
            <td width="25%"><img src="Che25/Che25.png" alt="SIGGRAPH'25" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Che25/Che25.pdf"  id="MCG_journal">
                    <papertitle>What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays</papertitle>
                  </a>
                  <br>
                  Chen, Matsuda, McElvain, Zhao, Wan, Sun*, <strong>Chapiro</strong>*
                  <br> * = equal contribution
                  <br><em>SIGGRAPH 2025 [conference]</em>
		  <br><a href="https://kenchen10.github.io/projects/sig25/index.html">project page </a> /
		  <a href="Che25/Che25_supplementary.pdf">supplementary </a> /
		  <a href="https://www.youtube.com/watch?v=iS_F3FhJpWc">video</a>
                </p>
                <p>
                  We investigated how the contrast and peak luminance of a display impact user preferences. Our precise large-scale study procedure allowed us to model the results in terms of absolute just-objectionable-difference (JOD) units. This data lets us to predict display quality, make decisions on design tradeoffs, and more.
		</p>
              </td>
            </tr>
				

	  <tr>
            <td width="25%"><img src="ke25/ke25.png" alt="NTIRE'25" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="ke25/ke25.pdf"  id="MCG_journal">
                    <papertitle>Training Neural Networks on RAW and HDR Images for Restoration Tasks</papertitle>
                  </a>
                  <br>
                  Ke, Luo, Xiang, Fan, Ranjan, <strong>Chapiro</strong>, Mantiuk
                  <br>
                  <em>NTIRE 2025 [CVPR workshop]</em>
		<br> <a href="#">presentation (tbd)</a>
                </p>
                <p>
                  Most content online is in a display-encoded color space (scaled 0-1). This is different from HDR/RAW content, typically scaled linearly in proportion to physical luminance. As a result, popular loss functions produce worse results for HDR/RAW, and there's no consensus on how to do better. We demonstrate that a 1-line change (applying a standard transfer function) can improve network performance for HDR/RAW image restoration tasks by 2-9 dB.
		</p>
              </td>
            </tr>
		

	  <tr>
            <td width="25%"><img src="Cha24/Cha24.jpg" alt="SIGGRAPH Asia'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Cha24/Cha24.pdf"  id="MCG_journal">
                    <papertitle>AR-DAVID: Augmented Reality Display Artifact Video Dataset</papertitle>
                  </a>
                  <br>
                  <strong>Chapiro</strong>, Kim, Asano, Mantiuk
                  <br>
                  <em>SIGGRAPH Asia 2024 [journal]</em>
		  <br> <a style="color:red"> Best paper award honorable mention </a> <br>
		  <a href="https://www.cl.cam.ac.uk/research/rainbow/projects/ardavid/">project page</a> /
		  <a href="https://github.com/gfxdisp/AR-DAVID/">GitHub</a> /
		  <a href="Cha24/IMG_2614.jpeg">award</a> /
		  <a href="https://youtu.be/DUCRmwqt-PE">video</a> /
		<a href="https://youtu.be/PTRAsj7QIpc?si=gSAV9CJ2PzUZHi6p">presentation</a>
                </p>
                <p>
                  The perception of distortions in augmented reality (AR) is very different than on traditional displays because of the presence of very bright see-through ambient conditions. We quantified this effect by conducting the first large-scale user study on the visibility of display artifacts in an AR setting. These results can help researchers understand visibility in AR and develop computational methods for this new, exciting platform.
		</p>
              </td>
            </tr>

		

	  <tr>
            <td width="25%"><img src="Jia24/Jia24.jpg" alt="SIGGRAPH Asia'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Jia24/Jia24.pdf"  id="MCG_journal">
                    <papertitle>FaceMap: Distortion-Driven Perceptual Facial Saliency Maps</papertitle>
                  </a>
                  <br>
                  Jiang, Venkateshan, Nam, Chen, Bachy, Bazin, <strong>Chapiro</strong>
                  <br>
                  <em>SIGGRAPH Asia 2024 [conference]</em><br>
		  <a href="https://github.com/facebookresearch/FaceMap?tab=readme-ov-file">github </a> /
		  <a href="Jia24/Jia24sup.pdf">supplementary </a> /
		  <a href="https://youtu.be/mPH4gJX6az0">video</a> /
		  <a href="https://www.youtube.com/watch?v=dweZTTSO0CI">presentation</a>
			
                </p>
                <p>
                  We created a saliency map for the human face, based on how visible perceptual distortions are per region. Our results are intuitive (e.g. texture distortions are especially visible on the eyes, and geometry distortions on the nose), and also provide a robust quantifiable prior that can be integrated into graphics pipelines for optimal resource distribution when rendering human faces. We demonstrate this via applications, like saliency-driven 3D Gaussian Splatting avatars.
		</p>
              </td>
            </tr>

			

	  <tr>
            <td width="25%"><img src="Che24b/Che24b.JPG" alt="TIP'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Che24b/Che24b.pdf"  id="MCG_journal">
                    <papertitle>Subjective and Objective Quality Assessment of Rendered Human Avatar Videos in Virtual Reality</papertitle>
                  </a>
                  <br>
                  Chen, Saha, <strong>Chapiro</strong>, Haene, Bazin, Qiu, Zanetti, Katsavounidis, Bovik
                  <br>
                  <em>Transactions on Image Processing 2024 [journal]</em><br>
		  <a href="Che24b/Che24b.bib">bibtex</a> /
		  <a href="https://live.ece.utexas.edu/research/LIVE-Meta-rendered-human-avatar/index.html">project page</a>
                </p>
                <p>
                  We evaluated the perceived impact of several graphics distortions relevant for video streaming on full body human avatars. Creating large-scale datasets such as these for human avatars is important, because the perception of artifacts on human characters is different from other types of objects.
		</p>
              </td>
            </tr>		


	  <tr>
            <td width="25%"><img src="Che24/che24.png" alt="SIGGRAPH'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Che24/che24.pdf"  id="MCG_journal">
                    <papertitle>PEA-PODs: Perceptual Evaluation of Algorithms for Power Optimization in XR Displays</papertitle>
                  </a>
                  <br>
                  Chen, Wan, Matsuda, Ninan, <strong>Chapiro</strong>*, Sun*
                  <br>
			* = equal contribution
		  <br>
                  <em>SIGGRAPH 2024 [journal]</em>
		  <br> <a style="color:red"> Best paper award honorable mention </a> <br>	
                  <a href="Che24/che24.bib">bibtex</a> /
		  <a href="https://github.com/NYU-ICL/pea-pods">github</a> /
		  <a href="https://kenchen10.github.io/projects/sig24/index.html">project page</a> /
		  <a href="https://blog.siggraph.org/2024/06/siggraph-2024-technical-papers-awards-best-papers-honorable-mentions-and-test-of-time.html/">award</a>
                </p>
                <p>
                  In this work we explored several perceptual algorithms (PEAs) for power optimization of displays (PODs). Different methods may introduce alterations of different types, and save more or less power depending on the display technology. We compare like-to-like in a user study, obtaining interpretable results for perceptually subtle breakpoints on how much display power can be saved across a wide range of technologies: LCD, OLED, with or without eye tracking, and more.
		</p>
              </td>
            </tr>		


	  <tr>
            <td width="25%"><img src="Man24/man24.png" alt="SIGGRAPH'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Man24/man24.pdf"  id="MCG_journal">
                    <papertitle>ColorVideoVDP: A visual difference predictor for image, video and display distortions</papertitle>
                  </a>
                  <br>
                  Mantiuk, Hanji, Ashraf, Asano, <strong>Chapiro</strong>
                  <br>
                  <em>SIGGRAPH 2024 [journal]</em><br>
                  <a href="Man24/man24.bib">bibtex</a> /
		  <a href="https://github.com/gfxdisp/ColorVideoVDP">github</a> /
		  <a href="https://www.cl.cam.ac.uk/research/rainbow/projects/colorvideovdp//">project page</a>
                </p>
                <p>
                  This paper continues our research into perceptual video difference metrics. ColorVideoVDP is the first fully spatio-temporal color-aware metric. It models display geometry and photometry, produces an output in interpretable just-objectionable-difference units, and runs fast. Code is available on GitHub, and extensive documentation and comparisons with other methods in the literature can be seen on the project page.
		</p>
              </td>
            </tr>

		
	  <tr>
            <td width="25%"><img src="Cha24a/cha24a.png" alt="SID'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Cha24a/cha24a.pdf"  id="MCG_journal">
                    <papertitle>Visible Difference Predictors: A Class of Perception-Based Metrics</papertitle>
                  </a>
                  <br>
                  <strong>Chapiro</strong>, Hanji, Ashraf, Asano, Mantiuk
                  <br>
                  <em>Society for Information Display, Display Week 2024 invited paper</em><br>
                  <a href="Cha24a/cha24a.bib">bibtex</a> /
		  <a href="https://youtu.be/RNsMh-lhLcA">presentation video</a>
                </p>
                <p>
                  This short paper and presentation given at SID Display Week 2024 in San Jose gives a quick overview of different aspects of image/video difference metrics, and some of the philosophy behind our work on VDP metrics, in particular ColorVideoVDP.
		</p>
              </td>
            </tr>	
		

	  <tr>
            <td width="25%"><img src="Ash24/Ash24.gif" alt="JOV'24" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Ash24/Ashraf2024_castleCSF.pdf"  id="MCG_journal">
                    <papertitle>castleCSF â€” A contrast sensitivity function of color, area, spatiotemporal frequency, luminance and eccentricity</papertitle>
                  </a>
                  <br>
                  Ashraf, Mantiuk, <strong>Chapiro</strong>, Wuerger
                  <br>
                  <em>Journal of Vision 2024</em><br>
                  <a href="Ash24/Ash24.bibtex">bibtex</a> /
		  <a href="https://github.com/gfxdisp/castleCSF">code & data</a> /
		  <a href="https://www.cl.cam.ac.uk/research/rainbow/projects/castleCSF/">project page</a>
			
			
                </p>
                <p>
                  We followed up on our work unifying contrast sensitivity datasets from the literature by integrating color. This extends our data-driven CSF model to now cover color, area, spatiotemporal frequency, luminance and eccentricity. Our model performs better than any existing work, and importantly covers critical perceptual parameters necessary for display engineering and computer graphics tasks. Data, code, and additional information available on our project page.
		</p>
              </td>
            </tr>	


	  <tr>
            <td width="25%"><img src="Tar23/Tar23.png" alt="SIGGRAPH Asia'23" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Tar23/Tar23.pdf"  id="MCG_journal">
                    <papertitle>Perceptually Adaptive Real-Time Tone Mapping</papertitle>
                  </a>
                  <br>
                  Tariq, Matsuda, Penner, Jia, Lanman, Ninan, <strong>Chapiro</strong>
                  <br>
                  <em>SIGGRAPH Asia 2023 [conference]<br>
                  <a href="Tar23/Tar23.bib">bibtex</a> /
		  <a href="Tar23/Tar23.mp4">video</a>
                </p>
                <p>
                  We created a perceptual framework that determines the optimal parameters for a tone mapper in real time (>1ms per frame on Quest2). We use this system and the Starburst HDR VR prototype to demonstrate that content shown with an optimized version of the Photographic TMO is preferred to heuristics or unoptimized versions even when display luminance is reduced tenfold.
		</p>
              </td>
            </tr>	
		


	  <tr>
            <td width="25%"><img src="pio23/pio23.png" alt="SIGGRAPH'23" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="pio23/pio23.pdf"  id="MCG_journal">
                    <papertitle>Skin-Screen: A Computational Fabrication Framework for Color Tattoos</papertitle>
                  </a>
                  <br>
                  Piovarci, <strong>Chapiro</strong>, Bickel
                  <br>
                  <em>SIGGRAPH 2023 [journal]<br>
                  <a href="pio23/pio23.bib">bibtex</a> /
		  <a href="https://misop.github.io/projects/ComputationalTattoo/">project page</a>
                </p>
                <p>
		In this work, we examined tattoos through the lens of computational fabrication. To build our model, we created an automatic tattoo robot, and processes to generate synthetic skins to experiment on. We created a framework to predict and modify tattoo color for different skin tones, which we hope will lead to better tattoo quality for everyone. This work also has medical and robotics applications!
		</p>
              </td>
            </tr>	
		
	




	  <tr>
            <td width="25%"><img src="cha23/cha23.png" alt="EI HVEI'23" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="cha23/cha23.pdf"  id="MCG_journal"> <papertitle>Critical Flicker Frequency (CFF) at high luminance levels</papertitle> </a>
                  <br>
                  <strong>Chapiro</strong>, Matsuda, Ashraf, Mantiuk
                  <br>
                  <em>Human Vision and Electronic Imaging (HVEI) 2023 <br>
                  <a href="cha23/cha23.bib">bibtex</a> /
		  <a href="https://www.youtube.com/watch?v=n4XVhHK5zek&list=PLoksP178KYM5yIGcV_cEXvFH7S3C3izoZ&index=23">presentation video</a>
                </p>
                <p>
		Flicker is a common temporal artifact that is affected by many parameters like luminance and retinal eccentricity. We gathered a high-luminance dataset for flicker fusion thresholds, showing that the popular Ferry-Porter law does not generally hold above 1,000 nits, and the increase in sensitivity saturates.
		</p>
              </td>
            </tr>	
		
	




	  <tr>
            <td width="25%"><img src="ash23/ash23.png" alt="EI HVEI'23" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="ash23/ash23.pdf"  id="MCG_journal">
                    <papertitle>Modelling contrast sensitivity of discs</papertitle>
                  </a>
                  <br>
                  Ashraf, Mantiuk, <strong>Chapiro</strong>
                  <br>
                  <em>Human Vision and Electronic Imaging (HVEI) 2023 <br>
                  <a href="ash23/ash23.bib">bibtex</a>
                </p>
                <p>
		Studies on spatial and temporal sensitivity are often done using different types of stimuli. We studied how experiments conducted using discs can be predicted using data for Gabors. This can lead to more comprehensive models calibrated on both types of data.
		</p>
              </td>
            </tr>	
		
	



	  <tr>
            <td width="25%"><img src="Wol22/wol22.jpg" alt="SIGGRAPH Asia'22" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Wol22/Wol22.pdf"  id="MCG_journal">
                    <papertitle>Geo-metric: A Perceptual Dataset of Distortions on Faces</papertitle>
                  </a>
                  <br>
                  Wolski, Trutoiu, Dong, Shen, MacKenzie, <strong>Chapiro</strong>
                  <br>
                  <em>SIGGRAPH Asia 2022 [journal]<br>
                  <a href="Wol22/Wol22.bib">bibtex</a> /
		  <a href="https://youtu.be/FXwRsXQZi-U">video</a> /
		  <a href="https://youtu.be/EYfQrMbMlp8">presentation</a> /
		  <a href="https://github.com/facebookresearch/Geo-metric">code/data</a>
			
                </p>
                <p>
		We study the perception of geometric distortions. We create a novel demographically-balanced dataset of human faces, and find the perceived magnitudes of several relevant distortions through a large-scale subjective study.
		</p>
              </td>
            </tr>	
		
	



	  <tr>
            <td width="25%"><img src="Mat22b/mat22b_v2.png" alt="SIGGRAPH Asia'22" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Mat22b/Mat22b.pdf"  id="MCG_journal">
                    <papertitle>Realistic Luminance in VR</papertitle>
                  </a>
                  <br>
                  Matsuda*, <strong>Chapiro</strong>*, Zhao, Bachy, Lanman
                  <br>
			* = equal contribution
		  <br>
                  <em>SIGGRAPH Asia 2022 [conference]<br>
                  <a href="Mat22b/Mat22b.bib">bibtex</a> /
		  <a href="https://youtu.be/G07yjRUJDow">video</a> /
		  <a href="https://youtu.be/p-j31U-ulNQ">presentation</a> /
		  <a href="https://github.com/facebookresearch/RealisticLuminanceInVR">code/data</a>
			  
                </p>
                <p>
                  We used the Starburst HDR VR 20,000+ nits prototype display to run a study measuring user preferences for realism when immersed in natural scenes. We found that user preference extends beyond what is available in VR today, and changes significantly between indoor and outdoor scenes.
		</p>
              </td>
            </tr>	
		
	



	  <tr>
            <td width="25%"><img src="Mat22a/mat22a.png" alt="SIGGRAPH'22 E-tech" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Mat22a/mat22a.pdf"  id="MCG_journal">
                    <papertitle>HDR VR</papertitle>
                  </a>
                  <br>
                  Matsuda, Zhao, <strong>Chapiro</strong>, Smith, Lanman
                  <br>
                  <em>SIGGRAPH'22 E-tech
		  <br> <a style="color:red"> Best in show award </a> <br>
                  <a href="Mat22a/Mat22a.bib">bibtex</a> /
		  <a href="Mat22a/mat22a.m4v">video</a> / 
		  <a href="Mat22a/mat22a_award.jpg">award</a>
			
                </p>
                <p>
                  Our HDR VR prototype display can reach brightness values over 20,000 nits. This work won "best in show" in the Emerging Technologies section of SIGGRAPH'22, and has received widespread media attention: <a href="https://www.youtube.com/watch?v=x6AOwDttBsc&t=2668s">Adam Savage's Tested</a>, <a href="https://www.youtube.com/watch?v=6QIniko9uK8">CNET</a>, <a href="https://www.youtube.com/watch?v=KDUP14Y12Is">UploadVR</a>, <a href="https://www.digitaltrends.com/computing/meta-teases-first-hdr-vr-headset-holographic-displays/">DigitalTrends</a>, <a href="https://www.techradar.com/news/these-are-the-vr-headsets-meta-doesnt-want-you-to-use">TechRadar</a>, <a href="https://mashable.com/article/meta-virtual-reality-holocake-butterscotch-starburst">Mashable</a>, <a href="https://www.roadtovr.com/meta-vr-headset-prototypes-visual-fidelity-indistinguishable-from-reality/2/">RoadToVR</a>. 			
		</p>
              </td>
            </tr>	
		
	

	  <tr>
            <td width="25%"><img src="Man22/Man22.jpeg" alt="SIGGRAPH'22" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Man22/mantiuk2022_stelaCSF.pdf"  id="MCG_journal">
                    <papertitle>stelaCSF-A Unified Model of Contrast Sensitivity as the Function of Spatio-Temporal Frequency, Eccentricity, Luminance and Area</papertitle>
                  </a>
                  <br>
                  Mantiuk, Ashraf, <strong>Chapiro</strong>
                  <br>
                  <em>SIGGRAPH 2022 [journal]</em><br>
                  <a href="Man22/man22.bib">bibtex</a> /
		  <a href="https://github.com/gfxdisp/stelaCSF">code & data</a> /
		  <a href="https://www.cl.cam.ac.uk/research/rainbow/projects/stelaCSF/">project page</a>
			
			
                </p>
                <p>
                  We unified contrast sensitivity datasets from the literature, which allowed us to create the most comprehensive and precise CSF model to date. This new model can be used to improve many applications in visual computing, such as metrics. Data, code, and additional information available on our project page.
		</p>
              </td>
            </tr>	
		
	

	  <tr>
            <td width="25%"><img src="Man21/Man21.png" alt="SIGGRAPH'21" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Man21/mantiuk2021_VideoVDP.pdf"  id="MCG_journal">
                    <papertitle>FovVideoVDP: A Visible Difference Predictor for Wide Field-of-View Video</papertitle>
                  </a>
                  <br>
                  Mantiuk, Denes, <strong>Chapiro</strong>, Kaplanyan, Rufo, Bachy, Lian, Patney
                  <br>
                  <em>SIGGRAPH 2021 [journal]</em><br>
                  <a href="Man21/mantiuk2021_VideoVDP_supplementary.pdf">supplementary material</a>  /
                  <a href="Man21/man21.bib">bibtex</a> /
		  <a href="https://github.com/gfxdisp/FovVideoVDP">github</a> /
		  <a href="https://www.cl.cam.ac.uk/research/rainbow/projects/fovvideovdp/">project page</a>
			
			
                </p>
                <p>
                  We created a new foveated spatiotemporal metric, following the VDP line of work. This metric is fast, easy to use and has been carefully calibrated on several large datasets.
		</p>
              </td>
            </tr>	
		
		


          <tr>
            <td width="25%"><img src="Cha19a/Cha19a.png" alt="ACM TOG 2019" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Cha19a/cha19a.pdf"  id="MCG_journal">
                    <papertitle>A Luminance-Aware Model of Judder Perception</papertitle>
                  </a>
                  <br>
                  <strong>Chapiro</strong>, Atkins, Daly.
                  <br>
                  <em>ACM Transactions on Graphics (TOG), Presented at SIGGRAPH 2020</em><br>
		  <a href="https://dl.acm.org/citation.cfm?id=3338696">Link to ACM TOG</a> /
                  <a href="Cha19a/cha19a.zip">supplementary material</a>  /
                  <a href="Cha19a/cha19a.bib">bibtex</a> /
		  <a href="Cha19a/judder.m">code</a> /
		  <a href="https://youtu.be/8cy54on-a3Y">presentation video</a>
			
			
                </p>
                <p>
                  We studied the main perceptual components of judder, the perceptual artifact of non-smooth motion. In particular, adaptation luminance is a strong factor on judder that has changed significantly with modern generations of displays.
		</p>
	        <p><i> Errata: In the table of coefficients in Sec. A3, the two-before-last coefficient should be ~0 instead of 1.01. The coefficient is correctly written out in the supplementary material, but not in the manuscript.</i>
	        </p>
              </td>
            </tr>
		

          <tr>
            <td width="25%"><img src="Cha18a/Cha18a.png" alt="ACM TAP 2018" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Cha18a/Cha18a.pdf"  id="MCG_journal">
                    <papertitle>Influence of Screen Size and Field of View on Perceived Brightness</papertitle>
                  </a>
                  <br>
                  <strong>Chapiro</strong>, Kunkel, Atkins, Daly.
                  <br>
                  <em>ACM Transactions on Applied Perception (TAP) </em>, 2018<br>
		  <a href="https://dl.acm.org/citation.cfm?id=3190346">Link to ACM TAP</a> /
                  <a href="Cha18a/supplementary.zip">supplementary material</a>  /
                  <a href="Cha18a/Cha18a.bib">bibtex</a>
			
                </p>
                <p>
                  Author's version available here, link to ACM TAP version above. We studied the influence of screen size and distance on perceived brightness for screens as large as cinema and as small as mobile phones, an issue that affects artistic intent and appearance matching.
                </p>
              </td>
            </tr>


          <tr>
            <td width="25%"><img src="Zun15c/Zun15c.jpg" alt="CVMP2015" width="160" style="border-style: none">
              <td width="75%" valign="top">
                <p>
                  <a href="Zun15c/Zun15c.pdf"  id="MCG_journal">
                    <papertitle>Unfolding the 8-bit Era</papertitle>
                  </a>
                  <br>
                  Zund, Berard, <strong>Chapiro</strong>, Schmid, Ryffel, Bermano, Gross, Sumner.
                  <br>
                  <em>European Conference on Visual Media Production (CVMP) </em>, 2015<br>
                  <a href="https://cgl.ethz.ch/publications/papers/paperZun15c.php">project page</a>  /
                  <a href="Zun15c/Zun15c.bib">bibtex</a>
                </p>
                <p>
                  We created an immersive gaming system out of a legacy console. This work was presented at the Eurographics 2015 <a href="https://gtc.inf.ethz.ch/news-events/2015/05/eurographics-2015-dinner-party.html">banquet</a>, and the <a href="http://www.ludicious.ch/highlight/unfolding-8-bit-era/">Ludicious</a> game festival in Zurich. It also received wide media attention:
			 <a href="http://arstechnica.co.uk/gadgets/2015/12/8-players-8-projectors-8-bits-and-just-one-nintendo-entertainment-system">arstechnica</a>, <a href="https://www.engadget.com/2015-12-03-super-mario-bros-multiplayer.html">engadget</a>, <a href="https://www.20min.ch/story/retro-konsole-der-eth-begeistert-nerds-weltweit-291009190774">20minuten</a>, <a href="https://www.konbini.com/archive/disney-nes-huit-joueurs-360-degres/">konbini</a>, <a href="https://www.xataka.com/videojuegos/una-nes-supervitaminada-y-conectada-a-8-proyectores-nos-sorprende-con-soporte-para-8-jugadores">xataca</a>, <a href="https://www.gamedeveloper.com/programming/this-is-what-it-looks-like-to-play-8-player-co-op-i-mario-i-on-an-nes">gamedeveloper</a>, <a href="https://www.factornews.com/actualites/les-vendredis-a-la-mod-edition-fous-furieux-41225.html">factornews</a>, <a href="https://boingboing.net/2015/12/23/swiss-researchers-solve-side-s.html">boingboing</a>, and has over 200,000 views on <a href="https://www.youtube.com/watch?v=SyU7Fscd2KU">YouTube</a>.
                </p>
              </td>
            </tr>




            <tr>
              <td width="25%"><img src="Cha15b/Cha15b.jpg" alt="CGnA2015" width="160" style="border-style: none">
                <td width="75%" valign="top">
                  <p>
                    <a href="Cha15b/Cha15b.pdf"  id="MCG_journal">
                      <papertitle>Art-Directable/Continuous Dynamic Range Video</papertitle>
                    </a>
                    <br>
                    <strong>Chapiro</strong>, Aydin, Stefanoski, Croci, Smolic, Gross.
                    <br>
                    <em>Computers and Graphics (CG&A) </em>, 2015<br>
                    <a href="https://cgl.ethz.ch/publications/papers/paperCha15b.php">project page</a>  /
                    <a href="Cha15b/Cha15b.bib">bibtex</a>
                  </p>
                  <p>
                    We defined the production and distribution challenges facing the content creation industry in the current HDR landscape and proposed Continuous Dynamic Range video as a solution.
                  </p>
                </td>
              </tr>




              <tr>
                <td width="25%"><img src="MMSP15/MMSP15.jpg" alt="CGnA2015" width="160" style="border-style: none">
                  <td width="75%" valign="top">
                    <p>
                      <a href="MMSP15/MMSP15.pdf"  id="MCG_journal">
                        <papertitle>Video Content and Structure Description Based on Keyframes, Clusters and Storyboards</papertitle>
                      </a>
                      <br>
                      Junyent, Beltran, Farre, Pont-Tuset, <strong>Chapiro</strong>, Smolic.
                      <br>
                      <em>IEEE International Workshop on Multimedia Signal Processing (MMSP) </em>, 2015<br>
                      <a href="MMSP15/MMSP15.mp4">video</a>  /
                      <a href="MMSP15/MMSP15.bib">bibtex</a>
                    </p>
                    <p>
                      We developed a pipeline to segment and analyze video. Our technique could be applied for smarter editing.
                    </p>
                  </td>
                </tr>




                <tr>
                  <td width="25%"><img src="Cha15a/Cha15a.jpg" alt="CGnA2015" width="160" style="border-style: none">
                    <td width="75%" valign="top">
                      <p>
                        <a href="Cha15a/Cha15a.pdf"  id="MCG_journal">
                          <papertitle>Stereo from Shading</papertitle>
                        </a>
                        <br>
                        <strong>Chapiro</strong>, O'Sullivan, Jarosz, Gross, Smolic.
                        <br>
                        <em>Eurographics Symposium on Rendering (EGSR) </em>, 2015<br>
                        <a href="https://cgl.ethz.ch/publications/papers/paperCha15a.php">project page</a>  /
                        <a href="Cha15a/Cha15a.bib">bibtex</a> /
                        <a href="Cha15a/Cha15a.zip">supplementary</a>
                      </p>
                      <p>
                        We use non-photorealistic shading as an alternative 3D cue, augmenting the feeling of depth in stereoscopic images.
                      </p>
                    </td>
                  </tr>




                  <tr>
                    <td width="25%"><img src="Cha14b/Cha14b.jpg" alt="CGnA2015" width="160" style="border-style: none">
                      <td width="75%" valign="top">
                        <p>
                          <a href="Cha14b/Cha14b.pdf"  id="MCG_journal">
                            <papertitle>Perceptual Evaluation of Cardboarding in 3D Content Visualization</papertitle>
                          </a>
                          <br>
                          <strong>Chapiro</strong>, O'Sullivan, Jarosz, Gross, Smolic.
                          <br>
                          <em>ACM Symposium on Applied Perception (SAP)</em>, 2014<br>
                          <a href="https://cgl.ethz.ch/publications/papers/paperCha14b.php">project page</a>  /
                          <a href="Cha14b/Cha14b.bib">bibtex</a>
                        </p>
                        <p>
                          We conducted perceptual experiments to quantify cardboarding - an artifact that occurs when not enough depth is given to a stereoscopic 3D image region.
                        </p>
                      </td>
                    </tr>




                    <tr>
                      <td width="25%"><img src="Cha14a/Cha14a.jpg" alt="CGnA2015" width="160" style="border-style: none">
                        <td width="75%" valign="top">
                          <p>
                            <a href="Cha14a/Cha14a.pdf"  id="MCG_journal">
                              <papertitle>Optimizing Stereo-to-Multiview Conversion for Autostereoscopic Displays</papertitle>
                            </a>
                            <br>
                            <strong>Chapiro</strong>, Heinzle, Aydin, Poulakos, Zwicker, Smolic, Gross.
                            <br>
                            <em>Eurographics</em>, 2014<br>
                            <a href="https://cgl.ethz.ch/publications/papers/paperCha14a.php">project page</a>  /
                            <a href="Cha14a/Cha14a.bib">bibtex</a>
                          </p>
                          <p>
                            We measured perceptual aspects of autostereo content and created a depth re-mapping algorithm that tries to optimize content so that the most important regions have a fuller sense of depth while staying within the limits of the technology.
                          </p>
                        </td>
                      </tr>




                      <tr>
                        <td width="25%"><img src="Cas11/Cas11.jpg" alt="Cas11" width="160" style="border-style: none">
                          <td width="75%" valign="top">
                            <p>
                              <a href="Cas11/Cas11.pdf"  id="MCG_journal">
                                <papertitle>Towards Mobile HDR Video</papertitle>
                              </a>
                              <br>
                              Castro, <strong>Chapiro</strong>, Cicconet, Velho.
                              <br>
                              <em>extended abstract in Eurographics</em>, 2011<br>
                              <a href="Cas11/Cas11.mov">video</a>
                            </p>
                            <p>
                              We created a capture and processing pipeline to generate HDR video on a mobile phone by taking sequential multiple exposures.
                            </p>
                          </td>
                        </tr>




                        <tr>
                          <td width="25%"><img src="Mot09/Mot09.jpg" alt="Cas11" width="160" style="border-style: none">
                            <td width="75%" valign="top">
                              <p>
                                <a href="Mot09/Mot09.pdf"  id="MCG_journal">
                                  <papertitle>Detection of High Frequency Regions in Multiresolution</papertitle>
                                </a>
                                <br>
                                Mota, Perez, Castro, <strong>Chapiro</strong>, Vieira.
                                <br>
                                <em>International Conference on Image Processing (ICIP)</em>, 2009<br>
                              </p>
                              <p>
                                We improved our edge detector by using eigenvalues.
                              </p>
                            </td>
                          </tr>




                          <tr>
                            <td width="25%"><img src="Cas09/Cas09.jpg" alt="Cas9" width="160" style="border-style: none">
                              <td width="75%" valign="top">
                                <p>
                                  <a href="Cas09/Cas09.pdf"  id="MCG_journal">
                                    <papertitle>High Frequency Assessment from Multiresolution Analysis</papertitle>
                                  </a>
                                  <br>
                                  Castro, Perez, Mota, <strong>Chapiro</strong>, Vieira, Freire.
                                  <br>
                                  <em>International Conference on Computational Science (ICCS)</em>, 2009<br>
                                </p>
                                <p>
                                  We detected high frequencies using an orientation tensor and multiresolution.
                                </p>
                              </td>
                            </tr>

	 <tr>
        <td>
		<h3 style="margin-bottom:0px;">Posters:</h3>
        
        </td>
      </tr>	  
	  
	  
	  
	  
		<tr>
          <td width="25%"><img src="Hub15/Hub15.jpg" alt="Hub15" width="160" style="border-style: none">
          <td width="75%" valign="top">
          <p>
            <a href="Hub15/Hub15.pdf"  id="MCG_journal">
            <papertitle>The Influence of Visual Salience on Video Consumption Behavior A Survival Analysis Approach</papertitle>
            </a>
            <br>
Huber, Scheibehenne, <strong>Chapiro</strong>, Frey, Sumner.
<br>
  <em>ACM Web Science</em>, 2015<br>
   <a href="Hub15/Hub15_long.pdf">long paper version</a>
          </p>
          <p>
            We found that visual saliency can be used as a predictor for video watching behavior in online platforms.
          </p>
          </td>
        </tr>
	 	  
	  
	  
	  
		<tr>
          <td width="25%"><img src="Cha11/Cha11.jpg" alt="Cha11" width="160" style="border-style: none">
          <td width="75%" valign="top">
          <p>
            <a href="Cha11/Cha11.pdf"  id="MCG_journal">
            <papertitle>Filter Based Deghosting for Exposure Fusion Video</papertitle>
            </a>
            <br>
			<strong>Chapiro</strong>, Cicconet, Velho.
<br>
  <em>SIGGRAPH</em>, 2011<br>
   <a href="Cha11/Cha11.mp4">video</a>
          </p>
          <p>
            We used an aditional per-pixel parameter to avoid ghosting from motion when generating exposure-fusion videos on a mobile phone. (Student research competition semi-finalist)
          </p>
          </td>
        </tr>
	 	  
	  
	  
	  
		<tr>
          <td width="25%"><img src="Cas11b/Cas11b.jpg" alt="Cas11b" width="160" style="border-style: none">
          <td width="75%" valign="top">
          <p>
            <a href="Cas11b/Cas11b.pdf"  id="MCG_journal">
            <papertitle>Towards Mobile HDR Video</papertitle>
            </a>
            <br>
			Castro, <strong>Chapiro</strong>, Cicconet, Velho.
<br>
  <em>International Conference on Computational Photography (ICCP)</em>, 2011<br>
   <a href="Cas11b/Cas11b.mov">video</a>
          </p>
          <p>
            We created a capture and processing pipeline to generate HDR video on a mobile phone by taking sequential multiple exposures.
          </p>
          </td>
        </tr>
		
		
		
		
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        Additional publications available upon request.
        </td>
      </tr>
      </table>

                            <!--
                            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                            <tr>
                            <td>
                            <heading>Teaching</heading>
                          </td>
                        </tr>
                      </table>
                      <table width="100%" align="center" border="0" cellpadding="20">
                      <tr>
                      <td width="25%"><img src="pacman.jpg" alt="pacman" width="160" height="160"></td>
                      <td width="75%" valign="center">
                      <p>
                      <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">
                      <papertitle>CS188 - Fall 2010 (GSI)</papertitle>
                    </a>
                    <br><br>
                    <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">
                    <papertitle>CS188 - Spring 2011 (GSI)</papertitle>
                  </a>
                  <br>
                </p>
              </td>
            </tr>
          </table>
        -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
        <td>
        <br>
        <p align="right"><font size="2">
        Website template taken from <a href="https://people.eecs.berkeley.edu/~barron/">here</a>

      </font>
    </p>
  </td>
</tr>
</table>
<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));

</script> <script type="text/javascript">
try {
var pageTracker = _gat._getTracker("UA-7580334-1");
pageTracker._trackPageview();
} catch(err) {}
</script>
</td>
</tr>
</table>
</body>
</html>
